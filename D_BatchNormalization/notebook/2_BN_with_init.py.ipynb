{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)  # reproducible\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_num, layers=100, do_bn=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(neural_num) for i in range(layers)])\n",
    "        self.neural_num = neural_num\n",
    "        self.do_bn = do_bn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for (i, linear), bn in zip(enumerate(self.linears), self.bns):\n",
    "            x = linear(x)\n",
    "            if self.do_bn:\n",
    "                x = bn(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            if torch.isnan(x.std()): # 数据太大\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break\n",
    "            print(\"layers:{}, std:{}\".format(i, x.std().item()))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize(self, mode, std_init=1):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if mode ==\"normal\":\n",
    "                    # method 1\n",
    "                    nn.init.normal_(m.weight.data, std=std_init)    # normal: mean=0, std=1\n",
    "                elif mode == \"kaiming\":\n",
    "                    # method 2 kaiming\n",
    "                    nn.init.kaiming_normal_(m.weight.data)\n",
    "                else:\n",
    "                    print(\"不支持{}输入\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers:0, std:0.8165372014045715\n",
      "layers:1, std:0.8045248985290527\n",
      "layers:2, std:0.7726029753684998\n",
      "layers:3, std:0.767861545085907\n",
      "layers:4, std:0.8131486773490906\n",
      "layers:5, std:0.8373168706893921\n",
      "layers:6, std:0.880279541015625\n",
      "layers:7, std:0.7769911289215088\n",
      "layers:8, std:0.7530372142791748\n",
      "layers:9, std:0.7334368824958801\n",
      "layers:10, std:0.7272238731384277\n",
      "layers:11, std:0.7110223174095154\n",
      "layers:12, std:0.8030721545219421\n",
      "layers:13, std:0.7797623872756958\n",
      "layers:14, std:0.840962827205658\n",
      "layers:15, std:0.8629758954048157\n",
      "layers:16, std:0.7960706353187561\n",
      "layers:17, std:0.7957866787910461\n",
      "layers:18, std:0.8553910851478577\n",
      "layers:19, std:0.7684149742126465\n",
      "layers:20, std:0.683224081993103\n",
      "layers:21, std:0.6878578066825867\n",
      "layers:22, std:0.6844690442085266\n",
      "layers:23, std:0.6568384766578674\n",
      "layers:24, std:0.629176676273346\n",
      "layers:25, std:0.5879842638969421\n",
      "layers:26, std:0.5388146638870239\n",
      "layers:27, std:0.5111389756202698\n",
      "layers:28, std:0.48382148146629333\n",
      "layers:29, std:0.4974925220012665\n",
      "layers:30, std:0.48855072259902954\n",
      "layers:31, std:0.5077669024467468\n",
      "layers:32, std:0.563236653804779\n",
      "layers:33, std:0.5296769142150879\n",
      "layers:34, std:0.5346172451972961\n",
      "layers:35, std:0.4631376564502716\n",
      "layers:36, std:0.4379015862941742\n",
      "layers:37, std:0.43224093317985535\n",
      "layers:38, std:0.4902610778808594\n",
      "layers:39, std:0.49641793966293335\n",
      "layers:40, std:0.500572681427002\n",
      "layers:41, std:0.49516695737838745\n",
      "layers:42, std:0.5293552279472351\n",
      "layers:43, std:0.49320822954177856\n",
      "layers:44, std:0.50212162733078\n",
      "layers:45, std:0.5108123421669006\n",
      "layers:46, std:0.5104655027389526\n",
      "layers:47, std:0.5122273564338684\n",
      "layers:48, std:0.4938479959964752\n",
      "layers:49, std:0.4562891721725464\n",
      "layers:50, std:0.4740261733531952\n",
      "layers:51, std:0.46757200360298157\n",
      "layers:52, std:0.5129030346870422\n",
      "layers:53, std:0.5003901124000549\n",
      "layers:54, std:0.4710868299007416\n",
      "layers:55, std:0.46483680605888367\n",
      "layers:56, std:0.4586785137653351\n",
      "layers:57, std:0.4809072017669678\n",
      "layers:58, std:0.4331323504447937\n",
      "layers:59, std:0.4117872416973114\n",
      "layers:60, std:0.44534730911254883\n",
      "layers:61, std:0.42838728427886963\n",
      "layers:62, std:0.48678678274154663\n",
      "layers:63, std:0.48909953236579895\n",
      "layers:64, std:0.4524807631969452\n",
      "layers:65, std:0.4502179026603699\n",
      "layers:66, std:0.45888033509254456\n",
      "layers:67, std:0.4027228057384491\n",
      "layers:68, std:0.3987591862678528\n",
      "layers:69, std:0.4119347333908081\n",
      "layers:70, std:0.41448524594306946\n",
      "layers:71, std:0.41561850905418396\n",
      "layers:72, std:0.3951345682144165\n",
      "layers:73, std:0.36812344193458557\n",
      "layers:74, std:0.3638400435447693\n",
      "layers:75, std:0.3460376560688019\n",
      "layers:76, std:0.32647886872291565\n",
      "layers:77, std:0.34537702798843384\n",
      "layers:78, std:0.3707936108112335\n",
      "layers:79, std:0.3936428725719452\n",
      "layers:80, std:0.37010157108306885\n",
      "layers:81, std:0.3571118414402008\n",
      "layers:82, std:0.33186620473861694\n",
      "layers:83, std:0.35625171661376953\n",
      "layers:84, std:0.3705413043498993\n",
      "layers:85, std:0.3639734089374542\n",
      "layers:86, std:0.3952462077140808\n",
      "layers:87, std:0.3642081618309021\n",
      "layers:88, std:0.39546117186546326\n",
      "layers:89, std:0.3705512583255768\n",
      "layers:90, std:0.4176543653011322\n",
      "layers:91, std:0.40747520327568054\n",
      "layers:92, std:0.4235135614871979\n",
      "layers:93, std:0.4433625340461731\n",
      "layers:94, std:0.4176541268825531\n",
      "layers:95, std:0.3826444149017334\n",
      "layers:96, std:0.40442609786987305\n",
      "layers:97, std:0.4170725345611572\n",
      "layers:98, std:0.410343199968338\n",
      "layers:99, std:0.41611090302467346\n",
      "tensor([[0.0000, 0.0350, 0.1486,  ..., 0.0000, 0.2306, 0.6462],\n",
      "        [0.0000, 0.0000, 0.1503,  ..., 0.0000, 0.3366, 0.6406],\n",
      "        [0.0000, 0.0496, 0.1636,  ..., 0.0000, 0.3697, 0.8147],\n",
      "        ...,\n",
      "        [0.0000, 0.0138, 0.0832,  ..., 0.0000, 0.2081, 0.5549],\n",
      "        [0.0000, 0.0197, 0.2361,  ..., 0.0000, 0.4600, 0.7343],\n",
      "        [0.0000, 0.0408, 0.2224,  ..., 0.0000, 0.3907, 0.7327]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    neural_nums = 256\n",
    "    layer_nums = 100\n",
    "    batch_size = 16\n",
    "\n",
    "#     做BN是否apply和初始化combo进行比较。我们可以发现当使用BN层时，对于权重初始化是多少的影响不大。BN applied时。不需要精心的设置权重初始化\n",
    "\n",
    "    net = MLP(neural_nums, layer_nums, do_bn=False)      # 1. 无初始化； # 2. normal_初始化； # 3。 kaiming初始化\n",
    "#     net = MLP(neural_nums, layer_nums, do_bn=True)        # 4. BN+无初始化； 5. BN + normal; 6. BN + kaiming, 7. BN+1000\n",
    "#     net.initialize(\"normal\", std_init=1)\n",
    "#     net.initialize(\"normal\", std_init=10000)\n",
    "#     net.initialize(\"kaiming\")\n",
    "\n",
    "    inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1\n",
    "\n",
    "    output = net(inputs)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察神经网络神经元数据尺度变化\n",
    "\n",
    "<font  size=12 face=\"黑体\">\n",
    "    \n",
    "有无BN层 | 无初始化 | N(0, 1) | Kaiming初始化 | N(0, 10000)\n",
    ":-: | :-: | :-: | :-: | :-:\n",
    "无BN层| 1e-40 | NaN in 35 layers | 0.4 | NaN in 8 layers| \n",
    "有BN层 | 0.57 | 0.57 | 0.57 |0.57|\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
